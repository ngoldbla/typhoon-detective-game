# LLM Configuration
# You can use any OpenAI-compatible API endpoint

# API Key (required)
# For Typhoon: Get from https://opentyphoon.ai
# For OpenAI: Get from https://platform.openai.com
# For local models (Ollama): Not needed
LLM_API_KEY=your_api_key_here

# API Endpoint (optional)
# Default: https://api.opentyphoon.ai/v1/chat/completions
# OpenAI: https://api.openai.com/v1/chat/completions
# Anthropic (via proxy): Your proxy endpoint
# Ollama: http://localhost:11434/v1/chat/completions
# LM Studio: http://localhost:1234/v1/chat/completions
LLM_API_ENDPOINT=https://api.opentyphoon.ai/v1/chat/completions

# Model Name (optional)
# Default: typhoon-v2.1-12b-instruct
# OpenAI: gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# Anthropic: claude-3-5-sonnet-20241022, claude-3-opus-20240229
# Ollama: llama3.1, mistral, qwen2.5
# Any other OpenAI-compatible model
LLM_MODEL=typhoon-v2.1-12b-instruct

# Legacy support (will use LLM_API_KEY if this is not set)
# TYPHOON_API_KEY=your_api_key_here